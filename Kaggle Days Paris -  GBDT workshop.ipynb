{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "211025bc64645c80563e6f2c62e496251a4ed20e"
   },
   "source": [
    "![Kaggle Days Paris](https://kaggledays.com/wp-content/uploads/sites/2/2018/11/46508555_1939772529664297_1579296553191866368_n-1024x536.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c18aa72fa74e5da21a87608d5ac158282eb37755"
   },
   "source": [
    "# Competitive GBDT specification and optimization workshop\n",
    "\n",
    "## Instructors\n",
    "* Luca Massaron [@lmassaron](https://www.linkedin.com/in/lmassaron/) - Data Scientist / Author / Google Developer Expert in Machine Learning \n",
    "* Pietro Marinelli [@pietro-marinelli-0098b427](https://www.linkedin.com/in/pietro-marinelli-0098b427/) - Freelance Data Scientist\n",
    "\n",
    "## About the workshop\n",
    "\n",
    "...\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "...\n",
    "\n",
    "## Content\n",
    "...\n",
    "\n",
    "## Obtaining the Tutorial Material\n",
    "...\n",
    "\n",
    "## Local installation notes\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "db61f14fd91a1fc33afdb194138c908946021939"
   },
   "source": [
    "# Optimizing hyper-parameters\n",
    "\n",
    "The topic of this workshop and notebook is to illustrate how to best optimize the hyperparameters of a gradient boost model (lightGBM before all, but also XGBoost and CatBoost) in the a performing and efficient way, by comparing the strong and weak points of grid-search, random search and bayesian optimization, using Scikit-optimize.\n",
    "\n",
    "After forgetting about grid-search (feasible when the space of experiments is limited), the choice is to apply the simple, no-brainer, random search optimization or try some [Bayesian Optimization](https://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf) (BO) technique. \n",
    "\n",
    "Scikit-Optimize, or skopt, is a simple and efficient library to minimize (very) expensive and noisy black-box functions.\n",
    "It can be found at https://github.com/scikit-optimize/scikit-optimize/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5abe0cd47e94562c7a1248c6dc946146eb713d50",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Installing the most recent version of skopt directly from Github\n",
    "!pip install git+https://github.com/scikit-optimize/scikit-optimize.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# Importing core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import pprint\n",
    "import joblib\n",
    "\n",
    "# Suppressing warnings because of skopt verbosity\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Our example dataset\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "# Classifiers\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "\n",
    "# Hyperparameters distributions\n",
    "from scipy.stats import randint\n",
    "from scipy.stats import uniform\n",
    "\n",
    "# Model selection\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, mean_absolute_error\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# Skopt functions\n",
    "from skopt import BayesSearchCV\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt.utils import use_named_args\n",
    "from skopt.callbacks import DeadlineStopper # Stop the optimization before running out of a fixed budget of time.\n",
    "from skopt.callbacks import VerboseCallback # \n",
    "from skopt.callbacks import DeltaXStopper # Stop the optimization If the last two positions at which the objective has been evaluated are less than delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "02a9b6a1bf577931da26f5862d1631f0d7bd4478"
   },
   "source": [
    "Optimizing hyper-parameters requires time and resources. In order to speed up the demonstration a toy dataset will be used, the Boston Houseprice dataset for a classification task, to predicted the top 10% most expensive houses.\n",
    "\n",
    "The dataset presents information collected by the U.S Census Service concerning housing proces and conditions in the area of Boston Mass. Originally found in the [StatLib archive](http://lib.stat.cmu.edu/datasets/boston), the dataset has been used extensively throughout the literature to benchmark machine learning algorithms. The data was originally published by :\n",
    "> Harrison, D. and Rubinfeld, D.L. Hedonic prices and the demand for clean air', J. Environ. Economics & Management, vol.5, 81-102, 1978.\n",
    "\n",
    "The dataset contains 14 variabile relative to 506 house that were sold in the suburbs of Boston. Among the variables, the 14th, MEDV - Median value of owner-occupied homes in $1000's - is commonly used as a target for regression problems. In our example we will use it for classification, after binarizing it at the 90th percentile (also creating an unbalanced classification problem, since the positive cases are just 10 percent of the total). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ec69b7012e67494e560a75c09459d608f74be88a"
   },
   "outputs": [],
   "source": [
    "# Uploading the Boston dataset\n",
    "X, y = load_boston(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7853f7a2a67c58b41ff98efe77df437ebd57f15f"
   },
   "outputs": [],
   "source": [
    "# Transforming the problem into a classification (unbalanced)\n",
    "y_bin = (y > np.percentile(y, 90)).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a6be6080c9429abc5a041f9b6bf825635ddbdf09"
   },
   "source": [
    "# Optimizing Scikit-learn GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4dd7d7b1699775a7a4e08c553ab3ec93a516cf56"
   },
   "outputs": [],
   "source": [
    "# Reporting util for different optimizers\n",
    "def report_perf(optimizer, X, y, title, callbacks=None):\n",
    "    \"\"\"\n",
    "    A wrapper for measuring time and performances of different optmizers\n",
    "    \n",
    "    optimizer = a sklearn or a skopt optimizer\n",
    "    X = the training set \n",
    "    y = our target\n",
    "    title = a string label for the experiment\n",
    "    \"\"\"\n",
    "    start = time()\n",
    "    if callbacks:\n",
    "        optimizer.fit(X, y, callback=callbacks)\n",
    "    else:\n",
    "        optimizer.fit(X, y)\n",
    "    d=pd.DataFrame(optimizer.cv_results_)\n",
    "    best_score = optimizer.best_score_\n",
    "    best_score_std = d.iloc[optimizer.best_index_].std_test_score\n",
    "    best_params = optimizer.best_params_\n",
    "    print((title + \" took %.2f seconds,  candidates checked: %d, best CV score: %.3f \"\n",
    "           +u\"\\u00B1\"+\" %.3f\") % (time() - start, \n",
    "                                  len(optimizer.cv_results_['params']),\n",
    "                                  best_score,\n",
    "                                  best_score_std))    \n",
    "    print('Best parameters:')\n",
    "    pprint.pprint(best_params)\n",
    "    print()\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9600a2f3b8baf43e0d86bf856621b206d0aedc9f"
   },
   "outputs": [],
   "source": [
    "# Converting average precision score into a scorer suitable for model selection\n",
    "avg_prec = make_scorer(average_precision_score, greater_is_better=True, needs_proba=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ab53b3efa1a0412bf7d02544e94143f609a303fc"
   },
   "outputs": [],
   "source": [
    "# Setting a 5-fold stratified cross-validation (note: shuffle=True)\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "# A Scikit-learn GBM classifier\n",
    "clf = GradientBoostingClassifier(n_estimators=20, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9746d8b7103c2a0fc1aeb1c04d9135807bf7bef3"
   },
   "outputs": [],
   "source": [
    "# GridSearchCV needs a predefined plan of the experiments\n",
    "\n",
    "grid_search = GridSearchCV(clf, \n",
    "                           param_grid={\"learning_rate\": [0.01, 1.0],\n",
    "                                       \"n_estimators\": [10, 500],\n",
    "                                       \"subsample\": [1.0, 0.5],\n",
    "                                       \"min_samples_split\": [2, 10],\n",
    "                                       \"min_samples_leaf\": [1, 10],\n",
    "                                       \"max_features\": ['sqrt', 'log2', None]\n",
    "                                       },\n",
    "                           n_jobs=-1,\n",
    "                           cv=skf,\n",
    "                           scoring=avg_prec,\n",
    "                           iid=False,\n",
    "                           return_train_score=False)\n",
    "\n",
    "best_params = report_perf(grid_search, X, y_bin,'GridSearchCV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "498623fb37dd803dd1edf5920e768e4b67ceef0a"
   },
   "outputs": [],
   "source": [
    "# RandomizedSearchCV needs the distribution of the experiments to be tested\n",
    "\n",
    "random_search = RandomizedSearchCV(clf, \n",
    "                                   param_distributions={\"learning_rate\": uniform(0.01, 1.0),\n",
    "                                                        \"n_estimators\": randint(10, 500),\n",
    "                                                        \"subsample\": uniform(0.5, 0.5),\n",
    "                                                        \"min_samples_split\": randint(2, 10),\n",
    "                                                        \"min_samples_leaf\": randint(1, 10),\n",
    "                                                        \"max_features\": ['sqrt', 'log2', None]\n",
    "                                       },\n",
    "                                   n_iter=40,\n",
    "                                   n_jobs=-1,\n",
    "                                   cv=skf,\n",
    "                                   scoring=avg_prec,\n",
    "                                   iid=False,\n",
    "                                   return_train_score=False,\n",
    "                                   random_state=0)\n",
    "\n",
    "best_params = report_perf(random_search, X, y_bin, 'RandomizedSearchCV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cca6073d5c1d67dd11ab6b8f77800d77ff044cca"
   },
   "outputs": [],
   "source": [
    "# also BayesSearchCV needs to work on the distributions of the experiments but it is less sensible to them\n",
    "\n",
    "search_spaces = {\"learning_rate\": Real(0.01, 1.0),\n",
    "                 \"n_estimators\": Integer(10, 500),\n",
    "                 \"subsample\": Real(0.5, 1.0),\n",
    "                 \"min_samples_split\": Integer(2, 10),\n",
    "                 \"min_samples_leaf\": Integer(1, 10),\n",
    "                 \"max_features\": Categorical(categories=['sqrt', 'log2', None])}\n",
    "\n",
    "for baseEstimator in ['GP', 'RF', 'ET', 'GBRT']:\n",
    "    opt = BayesSearchCV(clf,\n",
    "                        search_spaces,\n",
    "                        scoring=avg_prec,\n",
    "                        cv=skf,\n",
    "                        n_iter=40,\n",
    "                        n_jobs=-1,\n",
    "                        return_train_score=False,\n",
    "                        optimizer_kwargs={'base_estimator': baseEstimator},\n",
    "                        random_state=4)\n",
    "    \n",
    "    best_params = report_perf(opt, X, y_bin,'BayesSearchCV_'+baseEstimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4218521e57192fa23c0ab536653f3d07d04e2402"
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "def onstep(res):\n",
    "    global counter\n",
    "    args = res.x\n",
    "    x0 = res.x_iters\n",
    "    y0 = res.func_vals\n",
    "    print('Last eval: ', x0[-1], \n",
    "          ' - Score ', y0[-1])\n",
    "    print('Current iter: ', counter, \n",
    "          ' - Score ', res.fun, \n",
    "          ' - Args: ', args)\n",
    "    joblib.dump((x0, y0), 'checkpoint.pkl')\n",
    "    counter = counter+1\n",
    "    \n",
    "dimensions = [Real(0.01, 1.0, name=\"learning_rate\"),\n",
    "              Integer(10, 500, name=\"n_estimators\"),\n",
    "              Real(0.5, 1.0, name=\"subsample\"),\n",
    "              Integer(2, 10, name=\"min_samples_split\"),\n",
    "              Integer(1, 10, name=\"min_samples_leaf\"),\n",
    "              Categorical(categories=['sqrt', 'log2', None], name=\"max_features\")]\n",
    "\n",
    "def make_objective(model, X, y, space, cv, scoring):\n",
    "    @use_named_args(space)\n",
    "    def objective(**params):\n",
    "        model.set_params(**params)\n",
    "\n",
    "        return -np.mean(cross_val_score(model, \n",
    "                                        X, y, \n",
    "                                        cv=cv, \n",
    "                                        n_jobs=-1,\n",
    "                                        scoring=scoring))\n",
    "\n",
    "    return objective\n",
    "\n",
    "objective = make_objective(clf,\n",
    "                           X, y_bin,\n",
    "                           space=dimensions,\n",
    "                           cv=skf,\n",
    "                           scoring=avg_prec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "88361c5f2f425f8cbf334d8b494692405e8c0b86"
   },
   "outputs": [],
   "source": [
    "gp_round = gp_minimize(func=objective,\n",
    "                       dimensions=dimensions,\n",
    "                       acq_func='gp_hedge', # Expected Improvement.\n",
    "                       n_calls=10,\n",
    "                       callback=[onstep],\n",
    "                       random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "de1369d11ea6a3bc79328bc2f2e14dd2621f4686"
   },
   "outputs": [],
   "source": [
    "x0, y0 = joblib.load('checkpoint.pkl')\n",
    "\n",
    "gp_round = gp_minimize(func=objective,\n",
    "                       x0=x0,              # already examined values for x\n",
    "                       y0=y0,              # observed values for x0\n",
    "                       dimensions=dimensions,\n",
    "                       acq_func='gp_hedge', # Expected Improvement.\n",
    "                       n_calls=10,\n",
    "                       callback=[onstep],\n",
    "                       random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2826f2d0f47b973052a81643498617f3b6f1cbd1"
   },
   "outputs": [],
   "source": [
    "best_parameters = gp_round.x\n",
    "best_result = gp_round.fun\n",
    "print(best_parameters, best_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1a82e07c7940ff0587db89b8ceba55be99564fea"
   },
   "source": [
    "# A Practical Example: Optimizing LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "50ecf1e5429ea895469fb93055c033424b52f71f"
   },
   "source": [
    "The high-performance LightGBM algorithm is capable of being distributed and of fast-handling large amounts of data. It has been developed by a team at Microsoft as an open source project ([GitHub page](https:/ / github. com/ Microsoft/ LightGBM) / [academic paper](https:/ / papers. nips. cc/ paper/ 6907- lightgbm- a- highly- efficientgradient-boosting- decision- tree)). \n",
    "\n",
    "LightGBM is based on decision trees, as well as XGBoost, yet it follows a different strategy.\n",
    "Whereas XGBoost uses decision trees to split on a variable and exploring different cuts at that variable (the level-wise tree growth strategy), LightGBM concentrates on a split and goes on splitting from there in order to achieve a better fitting (this is the leaf-wise tree\n",
    "growth strategy). This allows LightGBM to reach first and fast a good fit of the data, and to generate alternative solutions compared to XGBoost (which is good, if you expect to blend, i.e. average, the two solutions together in order to reduce the variance of the estimated). Algorithmically talking, figuring out as a graph the structures of cuts operated by a decision tree, XGBoost peruses a breadth-first search (BFS), whereas LightGBM a depthfirst search (DFS).\n",
    "\n",
    "Tuning LightGBM may appear daunting with more than a [hundred parameters](https://github.com/Microsoft/LightGBM/blob/master/docs/Parameters.rst) to fix.\n",
    "\n",
    "If you want to know more about LightGBM:\n",
    "* [https://github.com/Microsoft/LightGBM/blob/master/examples/python-guide/advanced_example.py](https://github.com/Microsoft/LightGBM/blob/master/examples/python-guide/advanced_example.py)\n",
    "* [https://lightgbm.readthedocs.io/en/latest/Python-API.htm](https://lightgbm.readthedocs.io/en/latest/Python-API.html)l\n",
    "* [https://lightgbm.readthedocs.io/en/latest/Parameters.html](https://lightgbm.readthedocs.io/en/latest/Parameters.html)\n",
    "* ALBERTO, Boschetti; MASSARON, Luca. Python data science essentials: become an efficient data science practitioner by understanding Python's key concepts. Packt Publ., 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2d1d9fbdb869a040c93f1ffacf59c14f3e473264"
   },
   "outputs": [],
   "source": [
    "clf = lgb.LGBMClassifier(boosting_type='gbdt',\n",
    "                         class_weight='balanced',\n",
    "                         objective='binary',\n",
    "                         n_jobs=1, \n",
    "                         verbose=0)\n",
    "\n",
    "search_spaces = {\n",
    "        'learning_rate': Real(0.01, 1.0, 'log-uniform'),\n",
    "        'num_leaves': Integer(2, 500),\n",
    "        'max_depth': Integer(0, 500),\n",
    "        'min_child_samples': Integer(0, 200),\n",
    "        'max_bin': Integer(100, 100000),\n",
    "        'subsample': Real(0.01, 1.0, 'uniform'),\n",
    "        'subsample_freq': Integer(0, 10),\n",
    "        'colsample_bytree': Real(0.01, 1.0, 'uniform'),\n",
    "        'min_child_weight': Integer(0, 10),\n",
    "        'subsample_for_bin': Integer(100000, 500000),\n",
    "        'reg_lambda': Real(1e-9, 1000, 'log-uniform'),\n",
    "        'reg_alpha': Real(1e-9, 1.0, 'log-uniform'),\n",
    "        'scale_pos_weight': Real(1e-6, 500, 'log-uniform'),\n",
    "        'n_estimators': Integer(10, 10000)        \n",
    "        }\n",
    "\n",
    "opt = BayesSearchCV(clf,\n",
    "                    search_spaces,\n",
    "                    scoring=avg_prec,\n",
    "                    cv=skf,\n",
    "                    n_iter=40,\n",
    "                    n_jobs=-1,\n",
    "                    return_train_score=False,\n",
    "                    refit=True,\n",
    "                    optimizer_kwargs={'base_estimator': 'GP'},\n",
    "                    random_state=22)\n",
    "    \n",
    "best_params = report_perf(opt, X, y_bin,'LightGBM', \n",
    "                          callbacks=[DeltaXStopper(0.001), \n",
    "                                     DeadlineStopper(60*5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "555349ad8b39ec46807a3a27a444024506404cb0"
   },
   "source": [
    "## Controlling the time cost of Bayesian optimization\n",
    "\n",
    "Running a single LightGBM model could take long time and in a BO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5e4e99b9b5c3d22ae5afdf8e2437c66e3c84cefc"
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "\n",
    "clf = lgb.LGBMClassifier(boosting_type='gbdt',\n",
    "                         class_weight='balanced',\n",
    "                         objective='binary',\n",
    "                         n_jobs=1, \n",
    "                         verbose=0)\n",
    "\n",
    "dimensions = [Real(0.01, 1.0, 'log-uniform', name='learning_rate'),\n",
    "              Integer(2, 500, name='num_leaves'),\n",
    "              Integer(0, 500, name='max_depth'),\n",
    "              Integer(0, 200, name='min_child_samples'),\n",
    "              Integer(100, 100000, name='max_bin'),\n",
    "              Real(0.01, 1.0, 'uniform', name='subsample'),\n",
    "              Integer(0, 10, name='subsample_freq'),\n",
    "              Real(0.01, 1.0, 'uniform', name='colsample_bytree'),\n",
    "              Integer(0, 10, name='min_child_weight'),\n",
    "              Integer(100000, 500000, name='subsample_for_bin'),\n",
    "              Real(1e-9, 1000, 'log-uniform', name='reg_lambda'),\n",
    "              Real(1e-9, 1.0, 'log-uniform', name='reg_alpha'),\n",
    "              Real(1e-6, 500, 'log-uniform', name='scale_pos_weight'),\n",
    "              Integer(10, 10000, name='n_estimators')]\n",
    "\n",
    "objective = make_objective(clf,\n",
    "                           X, y_bin,\n",
    "                           space=dimensions,\n",
    "                           cv=skf,\n",
    "                           scoring=avg_prec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6de6368797702d3944fcb35082984be4910cfbbe"
   },
   "outputs": [],
   "source": [
    "gp_round = gp_minimize(func=objective,\n",
    "                       dimensions=dimensions,\n",
    "                       acq_func='gp_hedge', # Expected Improvement.\n",
    "                       n_calls=10, # Minimum is 10 calls\n",
    "                       callback=[onstep],\n",
    "                       random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4ba09b481009f9e87378aae6c4f17024878d31e1"
   },
   "outputs": [],
   "source": [
    "x0, y0 = joblib.load('checkpoint.pkl')\n",
    "\n",
    "gp_round = gp_minimize(func=objective,\n",
    "                       x0=x0,              # already examined values for x\n",
    "                       y0=y0,              # observed values for x0\n",
    "                       dimensions=dimensions,\n",
    "                       acq_func='gp_hedge', # Expected Improvement.\n",
    "                       n_calls=10,\n",
    "                       #callback=[onstep],\n",
    "                       random_state=3)\n",
    "\n",
    "best_parameters = gp_round.x\n",
    "best_result = gp_round.fun\n",
    "print(best_parameters, best_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "561467430472a5562fa25d04d2e3c4f311800ee6"
   },
   "source": [
    "# Practical example: Optimizing XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3de7945a9531bb50d0817da3a5d9d798443495ba"
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c899ead400eb74ce87a9176aca1a47d1fcbf5946"
   },
   "outputs": [],
   "source": [
    "clf = xgb.XGBClassifier(\n",
    "        n_jobs = 1,\n",
    "        objective = 'binary:logistic',\n",
    "        silent=1,\n",
    "        tree_method='approx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "71b250ac2647634c4528c4c3a55de66b66127144"
   },
   "outputs": [],
   "source": [
    "search_spaces = {'learning_rate': Real(0.01, 1.0, 'log-uniform'),\n",
    "                 'min_child_weight': Integer(0, 10),\n",
    "                 'max_depth': Integer(0, 50),\n",
    "                 'max_delta_step': Integer(0, 20),\n",
    "                 'subsample': Real(0.01, 1.0, 'uniform'),\n",
    "                 'colsample_bytree': Real(0.01, 1.0, 'uniform'),\n",
    "                 'colsample_bylevel': Real(0.01, 1.0, 'uniform'),\n",
    "                 'reg_lambda': Real(1e-9, 1000, 'log-uniform'),\n",
    "                 'reg_alpha': Real(1e-9, 1.0, 'log-uniform'),\n",
    "                 'gamma': Real(1e-9, 0.5, 'log-uniform'),\n",
    "                 'min_child_weight': Integer(0, 5),\n",
    "                 'n_estimators': Integer(50, 100),\n",
    "                 'scale_pos_weight': Real(1e-6, 500, 'log-uniform')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5ef9b0986d0677d866304c50a3c503b0bc4a4f40"
   },
   "outputs": [],
   "source": [
    "opt = BayesSearchCV(clf,\n",
    "                    search_spaces,\n",
    "                    scoring=avg_prec,\n",
    "                    cv=skf,\n",
    "                    n_iter=40,\n",
    "                    n_jobs=-1,\n",
    "                    return_train_score=False,\n",
    "                    refit=True,\n",
    "                    optimizer_kwargs={'base_estimator': 'GP'},\n",
    "                    random_state=22)\n",
    "    \n",
    "best_params = report_perf(opt, X, y_bin,'XGBoost',                           \n",
    "                          callbacks=[DeltaXStopper(0.001), \n",
    "                                     DeadlineStopper(60*5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f8d79cc09922ea88e960afdd9fadc021dc71ce1c"
   },
   "source": [
    "# Practical Example: Optimizing CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fcd4d562c7bcf554d4b3b71df94b4eab9a92073b"
   },
   "outputs": [],
   "source": [
    "!pip install catboost -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "24c3a9bdb05e58c83d3ffd51be6163f43d26f1aa"
   },
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "clf = CatBoostClassifier(loss_function='Logloss',\n",
    "                         verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "277816facc46abcc9581dd6a485cd519130656b2"
   },
   "outputs": [],
   "source": [
    "search_spaces = {'iterations': Integer(10, 100),\n",
    "                 'depth': Integer(1, 8),\n",
    "                 'learning_rate': Real(0.01, 1.0, 'log-uniform'),\n",
    "                 'random_strength': Real(1e-9, 10, 'log-uniform'),\n",
    "                 'bagging_temperature': Real(0.0, 1.0),\n",
    "                 'border_count': Integer(1, 255),\n",
    "                 'ctr_border_count': Integer(1, 255),\n",
    "                 'l2_leaf_reg': Integer(2, 30),\n",
    "                 'scale_pos_weight':Real(0.01, 1.0, 'uniform')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "816a4893a523ec3d9a155d6a095e0c023666b5d8"
   },
   "outputs": [],
   "source": [
    "opt = BayesSearchCV(clf,\n",
    "                    search_spaces,\n",
    "                    scoring=avg_prec,\n",
    "                    cv=skf,\n",
    "                    n_iter=40,\n",
    "                    n_jobs=1,  # use just 1 job with CatBoost in order to avoid segmentation fault\n",
    "                    return_train_score=False,\n",
    "                    refit=True,\n",
    "                    optimizer_kwargs={'base_estimator': 'GP'},\n",
    "                    random_state=22)\n",
    "\n",
    "best_params = report_perf(opt, X, y_bin,'CatBoost', \n",
    "                          callbacks=[DeltaXStopper(0.001), \n",
    "                                     DeadlineStopper(60*5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "87376ddce79cff6e491ae0bf2f77ecb42379e9b3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
