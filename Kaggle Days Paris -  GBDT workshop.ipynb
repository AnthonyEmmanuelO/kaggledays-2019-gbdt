{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "211025bc64645c80563e6f2c62e496251a4ed20e"
   },
   "source": [
    "![Kaggle Days Paris](https://kaggledays.com/wp-content/uploads/sites/2/2018/11/46508555_1939772529664297_1579296553191866368_n-1024x536.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c18aa72fa74e5da21a87608d5ac158282eb37755"
   },
   "source": [
    "# Competitive GBDT Specification and Optimization Workshop\n",
    "\n",
    "\n",
    "## Instructors\n",
    "* Luca Massaron [@lmassaron](https://www.linkedin.com/in/lmassaron/) - Data Scientist / Author / Google Developer Expert in Machine Learning \n",
    "* Pietro Marinelli [@pietro-marinelli-0098b427](https://www.linkedin.com/in/pietro-marinelli-0098b427/) - Freelance Data Scientist\n",
    "\n",
    "## About the workshop\n",
    "\n",
    "Gradient Boosting Decision Trees (GBDT) presently represent the state of the art for building predictors for flat table data. However, they seldom perform the best out-of-the-box (using default values) because of the many hyper-parameters to tune. Especially in the most recent GBDT implementations, such as LightGBM, the over-sophistication of hyper-parameters renders finding the optimal settings by hand or simple grid search difficult because of high combinatorial complexity and long running times for experiments. \n",
    "\n",
    "[Random Optimization](https://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf) (BERGSTRA, James; BENGIO, Yoshua. Random search for hyper-parameter optimization. Journal of Machine Learning Research, 2012, 13.Feb: 281-305.) and [Bayesian Optimization](https://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf) (SNOEK, Jasper; LAROCHELLE, Hugo; ADAMS, Ryan P. Practical bayesian optimization of machine learning algorithms. In: Advances in neural information processing systems. 2012. p. 2951-2959) are often the answer you'll find from experts.\n",
    "\n",
    "In this workshop we demonstrate how to use different optimization approaches based on [Scikit-Optimize](https://github.com/scikit-optimize/scikit-optimize), a library built on top of NumPy, SciPy and Scikit-Learn, and we present an easy and fast approach to set them ready and usable.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "You should be aware of the role and importance of hyper-parameter optimization in machine learning.  \n",
    "\n",
    "## Obtaining the Tutorial Material\n",
    "In order to make the workshop easily accessible, we are offering cloud access:\n",
    "* Using [Google Colab](https://colab.research.google.com/github/lmassaron/kaggledays-2019-gbdt/blob/master/Kaggle%20Days%20Paris%20-%20%20GBDT%20workshop.ipynb) \n",
    "* Using [Kaggle Kernels]()\n",
    "\n",
    "We also have a brief exercise that can be found at:\n",
    "* Using [Google Colab](https://colab.research.google.com/github/lmassaron/kaggledays-2019-gbdt/blob/master/Kaggle%20Days%20Paris%20-%20Skopt%20%2B%20CatBoost%20exercise.ipynb)\n",
    "* Using [Kaggle Kernels]()\n",
    "\n",
    "The solution can be found [here](https://github.com/lmassaron/kaggledays-2019-gbdt/blob/master/Kaggle%20Days%20Paris%20-%20Skopt%20%2B%20CatBoost%20solution.ipynb).\n",
    "\n",
    "All the materials can be cloned from Github at the [kaggledays-2019-gbdt](https://github.com/lmassaron/kaggledays-2019-gbdt) repository.\n",
    "\n",
    "## Local installation notes\n",
    "\n",
    "In order to successfully run this workshop on your local computer, you need a Python3 installation (we suggest installing the most recent [Anaconda](https://www.anaconda.com/download/) distribution) and at least the following packages:\n",
    "\n",
    "* numpy >= 1.15.4\n",
    "* pandas >= 0.23.4\n",
    "* scipy >= 1.1.0\n",
    "* skopt >= 0.5.2\n",
    "* sklearn >= 0.20.2\n",
    "* lightgbm >= 2.2.2\n",
    "* xgboost >= 0.81\n",
    "* catboost >= 0.12.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "db61f14fd91a1fc33afdb194138c908946021939"
   },
   "source": [
    "# Optimizing hyper-parameters\n",
    "\n",
    "The topic of this workshop is to illustrate how to best optimize the hyperparameters of a gradient boosting model (lightGBM before all, but also XGBoost and CatBoost) in a performing and efficient way. We will also compare the strong and weak points of different tuning approaches, such grid-search, random search and bayesian optimization by Scikit-optimize.\n",
    "\n",
    "Leaving apart grid-search (feasible only when the space of experiments is limited), the usual choice for the practitioner is to apply random search optimization or try some [Bayesian Optimization](https://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf) (BO) technique, which require a more complex setup. \n",
    "\n",
    "As for as BO, there are quite a few choices (for instance Hyperopt) but we decided for Scikit-Optimize, or skopt, because it is a simple and efficient library to minimize (very) expensive and noisy black-box functions and it works with an API similar to Scikit-learn. It can be found at https://github.com/scikit-optimize/scikit-optimize/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "5abe0cd47e94562c7a1248c6dc946146eb713d50",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/scikit-optimize/scikit-optimize.git\n",
      "  Cloning https://github.com/scikit-optimize/scikit-optimize.git to c:\\users\\luca\\appdata\\local\\temp\\pip-req-build-oglms7e5\n",
      "Requirement already satisfied (use --upgrade to upgrade): scikit-optimize==0+unknown from git+https://github.com/scikit-optimize/scikit-optimize.git in c:\\users\\luca\\anaconda3\\lib\\site-packages\n",
      "Requirement already satisfied: pyaml in c:\\users\\luca\\anaconda3\\lib\\site-packages (from scikit-optimize==0+unknown) (18.11.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\luca\\anaconda3\\lib\\site-packages (from scikit-optimize==0+unknown) (1.15.4)\n",
      "Requirement already satisfied: scipy>=0.14.0 in c:\\users\\luca\\anaconda3\\lib\\site-packages (from scikit-optimize==0+unknown) (1.1.0)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in c:\\users\\luca\\anaconda3\\lib\\site-packages (from scikit-optimize==0+unknown) (0.20.2)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\luca\\anaconda3\\lib\\site-packages (from pyaml->scikit-optimize==0+unknown) (3.13)\n",
      "Building wheels for collected packages: scikit-optimize\n",
      "  Running setup.py bdist_wheel for scikit-optimize: started\n",
      "  Running setup.py bdist_wheel for scikit-optimize: finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\Luca\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-fr8qpg45\\wheels\\11\\6f\\86\\2b772172db85ad0b4487d67e325e535ee8e7782b2a1dfcadf5\n",
      "Successfully built scikit-optimize\n"
     ]
    }
   ],
   "source": [
    "# Installing the most recent version of skopt directly from Github\n",
    "!pip install git+https://github.com/scikit-optimize/scikit-optimize.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: catboost in c:\\users\\luca\\anaconda3\\lib\\site-packages (0.12.2)\n",
      "Requirement already satisfied, skipping upgrade: enum34 in c:\\users\\luca\\anaconda3\\lib\\site-packages (from catboost) (1.1.6)\n",
      "Requirement already satisfied, skipping upgrade: six in c:\\users\\luca\\anaconda3\\lib\\site-packages (from catboost) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: pandas>=0.19.1 in c:\\users\\luca\\anaconda3\\lib\\site-packages (from catboost) (0.23.4)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.11.1 in c:\\users\\luca\\anaconda3\\lib\\site-packages (from catboost) (1.15.4)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in c:\\users\\luca\\anaconda3\\lib\\site-packages (from pandas>=0.19.1->catboost) (2.7.5)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2011k in c:\\users\\luca\\anaconda3\\lib\\site-packages (from pandas>=0.19.1->catboost) (2018.9)\n"
     ]
    }
   ],
   "source": [
    "# Assuring you have the most recent CatBoost release\n",
    "!pip install catboost -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# Importing core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import pprint\n",
    "import joblib\n",
    "\n",
    "# Suppressing warnings because of skopt verbosity\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Our example dataset\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Hyperparameters distributions\n",
    "from scipy.stats import randint\n",
    "from scipy.stats import uniform\n",
    "\n",
    "# Model selection\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, mean_absolute_error\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# Skopt functions\n",
    "from skopt import BayesSearchCV\n",
    "from skopt import gp_minimize # Bayesian optimization using Gaussian Processes\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt.utils import use_named_args # decorator to convert a list of parameters to named arguments\n",
    "from skopt.callbacks import DeadlineStopper # Stop the optimization before running out of a fixed budget of time.\n",
    "from skopt.callbacks import VerboseCallback # Callback to control the verbosity\n",
    "from skopt.callbacks import DeltaXStopper # Stop the optimization If the last two positions at which the objective has been evaluated are less than delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "02a9b6a1bf577931da26f5862d1631f0d7bd4478"
   },
   "source": [
    "Optimizing hyper-parameters requires time and resources. In order to speed up the demonstration we will be using a toy dataset, the Boston Houseprice dataset for a classification task, to predicted the top 10% most expensive houses.\n",
    "\n",
    "The dataset presents information collected by the U.S Census Service concerning housing proces and conditions in the area of Boston Mass. Originally found in the [StatLib archive](http://lib.stat.cmu.edu/datasets/boston), the dataset has been used extensively throughout the literature to benchmark machine learning algorithms. The data was originally published by :\n",
    "> Harrison, D. and Rubinfeld, D.L. Hedonic prices and the demand for clean air', J. Environ. Economics & Management, vol.5, 81-102, 1978.\n",
    "\n",
    "The dataset contains 14 variabile relative to 506 house that were sold in the suburbs of Boston. Among the variables, the 14th, MEDV - Median value of owner-occupied homes in $1000's - is commonly used as a target for regression problems. In our example we will use it for classification, after binarizing it at the 90th percentile (also creating an unbalanced classification problem, since the positive cases are just 10 percent of the total). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "ec69b7012e67494e560a75c09459d608f74be88a"
   },
   "outputs": [],
   "source": [
    "# Uploading the Boston dataset\n",
    "X, y = load_boston(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "7853f7a2a67c58b41ff98efe77df437ebd57f15f"
   },
   "outputs": [],
   "source": [
    "# Transforming the problem into a classification (unbalanced)\n",
    "y_bin = (y > np.percentile(y, 90)).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a6be6080c9429abc5a041f9b6bf825635ddbdf09"
   },
   "source": [
    "# Optimizing Scikit-learn GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearchCV, RandomizedSearchCV (from Scikit-learn) and BayesSearchCV (from Scikit-optimize) all have the same API. A wrapper can just put together optimization, callbacks, best results reporting and time monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "4dd7d7b1699775a7a4e08c553ab3ec93a516cf56"
   },
   "outputs": [],
   "source": [
    "# Reporting util for different optimizers\n",
    "def report_perf(optimizer, X, y, title, callbacks=None):\n",
    "    \"\"\"\n",
    "    A wrapper for measuring time and performances of different optmizers\n",
    "    \n",
    "    optimizer = a sklearn or a skopt optimizer\n",
    "    X = the training set \n",
    "    y = our target\n",
    "    title = a string label for the experiment\n",
    "    \"\"\"\n",
    "    start = time()\n",
    "    if callbacks:\n",
    "        optimizer.fit(X, y, callback=callbacks)\n",
    "    else:\n",
    "        optimizer.fit(X, y)\n",
    "    d=pd.DataFrame(optimizer.cv_results_)\n",
    "    best_score = optimizer.best_score_\n",
    "    best_score_std = d.iloc[optimizer.best_index_].std_test_score\n",
    "    best_params = optimizer.best_params_\n",
    "    print((title + \" took %.2f seconds,  candidates checked: %d, best CV score: %.3f \"\n",
    "           +u\"\\u00B1\"+\" %.3f\") % (time() - start, \n",
    "                                  len(optimizer.cv_results_['params']),\n",
    "                                  best_score,\n",
    "                                  best_score_std))    \n",
    "    print('Best parameters:')\n",
    "    pprint.pprint(best_params)\n",
    "    print()\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "9600a2f3b8baf43e0d86bf856621b206d0aedc9f"
   },
   "outputs": [],
   "source": [
    "# Converting average precision score into a scorer suitable for model selection\n",
    "avg_prec = make_scorer(average_precision_score, greater_is_better=True, needs_proba=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "ab53b3efa1a0412bf7d02544e94143f609a303fc"
   },
   "outputs": [],
   "source": [
    "# Setting a 5-fold stratified cross-validation (note: shuffle=True)\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Scikit-learn GBM classifier\n",
    "clf = GradientBoostingClassifier(n_estimators=20, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search exhaustively searches through the hyperparameters and is not feasible in high dimensional space\n",
    "This is a very simple algorithm and suffers from the curse of dimensionality, though it's embarrassingly parallel.\n",
    "\n",
    "Here we use, GridSearchCV, a function from Scikit-learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "9746d8b7103c2a0fc1aeb1c04d9135807bf7bef3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV took 72.11 seconds,  candidates checked: 96, best CV score: 0.909 ± 0.072\n",
      "Best parameters:\n",
      "{'learning_rate': 0.01,\n",
      " 'max_features': None,\n",
      " 'min_samples_leaf': 10,\n",
      " 'min_samples_split': 2,\n",
      " 'n_estimators': 500,\n",
      " 'subsample': 0.5}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# GridSearchCV needs a predefined plan of the experiments\n",
    "grid_search = GridSearchCV(clf, \n",
    "                           param_grid={\"learning_rate\": [0.01, 1.0],\n",
    "                                       \"n_estimators\": [10, 500],\n",
    "                                       \"subsample\": [1.0, 0.5],\n",
    "                                       \"min_samples_split\": [2, 10],\n",
    "                                       \"min_samples_leaf\": [1, 10],\n",
    "                                       \"max_features\": ['sqrt', 'log2', None]\n",
    "                                       },\n",
    "                           n_jobs=-1,\n",
    "                           cv=skf,\n",
    "                           scoring=avg_prec,\n",
    "                           iid=False, # just return the average score across folds\n",
    "                           return_train_score=False)\n",
    "\n",
    "best_params = report_perf(grid_search, X, y_bin,'GridSearchCV')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random search, which simply samples the search space randomly, is feasible in high dimensional spaces, and is widely used in practice. The downside of random search, however, is that it doesn’t use information from prior experiments to select the next setting.\n",
    "\n",
    "You simply need to be lucky to catch the right hyper-parameters, or just try more ;-).\n",
    "\n",
    "In fact, the 2×Random Search is the Random Search algorithm when it was allowed to sample two points for each point the other algorithms evaluated. While some authors have claimed that 2×Random Search is highly competitive with Bayesian Optimization methods, a [study by Google](http://delivery.acm.org/10.1145/3100000/3098043/p1487-golovin.pdf) (GOLOVIN, Daniel, et al. Google vizier: A service for black-box optimization. In: Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2017. p. 1487-1495) suggests that this is only true when the dimensionality of the problem is sufficiently high (e.g., over 16)\n",
    "\n",
    "RandomizedSearchCV is a function from Scikit-learn, though skopt has it own random optimizer, *[dummy_minimize](https://scikit-optimize.github.io/#skopt.dummy_minimize)*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "498623fb37dd803dd1edf5920e768e4b67ceef0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomizedSearchCV took 28.14 seconds,  candidates checked: 40, best CV score: 0.927 ± 0.097\n",
      "Best parameters:\n",
      "{'learning_rate': 0.10237388946089819,\n",
      " 'max_features': 'log2',\n",
      " 'min_samples_leaf': 2,\n",
      " 'min_samples_split': 9,\n",
      " 'n_estimators': 384,\n",
      " 'subsample': 0.8266004099285669}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RandomizedSearchCV needs the distribution of the experiments to be tested\n",
    "# If you can provide the right distribution, the sampling will lead to faster and better results.\n",
    "\n",
    "random_search = RandomizedSearchCV(clf, \n",
    "                                   param_distributions={\"learning_rate\": uniform(0.01, 1.0),\n",
    "                                                        \"n_estimators\": randint(10, 500),\n",
    "                                                        \"subsample\": uniform(0.5, 0.5),\n",
    "                                                        \"min_samples_split\": randint(2, 10),\n",
    "                                                        \"min_samples_leaf\": randint(1, 10),\n",
    "                                                        \"max_features\": ['sqrt', 'log2', None]\n",
    "                                       },\n",
    "                                   n_iter=40,\n",
    "                                   n_jobs=-1,\n",
    "                                   cv=skf,\n",
    "                                   scoring=avg_prec,\n",
    "                                   iid=False, # just return the average score across folds\n",
    "                                   return_train_score=False,\n",
    "                                   random_state=0)\n",
    "\n",
    "best_params = report_perf(random_search, X, y_bin, 'RandomizedSearchCV')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After examining the classical and most known approaches, it is time to dwelve into Bayesian optimization.\n",
    "\n",
    "Bayesian optimization is behind [Google Cloud Machine Learning Engine](https://cloud.google.com/blog/products/gcp/hyperparameter-tuning-cloud-machine-learning-engine-using-bayesian-optimization) services.\n",
    "\n",
    "The key idea behind Bayesian optimization is that we optimize a proxy function instead than the true objective function (what actually grid search and random search both do). This holds if testing the true objective function is costly (if it is not, then we simply go for random search :-))\n",
    "\n",
    "Bayesian search balances exploration against exploitation. At start it randomly explores, doing so it builds up a surrogate function of the objective. Based on that surrogate function it exploits an initial approximate knowledge of how the predictor works in order to sample more useful examples and minimize the cost function at a global level, not a local one.\n",
    "\n",
    "As the Bayesian part of the title suggests, we use priors in order to make smarter decisions about sampling during optimizing in order to reach a minimization faster by limiting the number of evaluations we need to make.\n",
    "\n",
    "Bayesian Optimization uses an acquisition function to tell us how promising an observation will be.\n",
    "In fact, to rule the tradeoff between exploration and exploitation, the algorithm defines an acquisition function that provides a single measure of how useful it would be to try any given point.\n",
    "\n",
    "From the figure taken from [Skopt API documentation](https://scikit-optimize.github.io/notebooks/bayesian-optimization.html), you can figure out that the surrogate function (the green dotted line, whose error band is represented by the light green area) has somehow approximated the true cost function (the red dotted line):\n",
    "\n",
    "![figure_1](https://scikit-optimize.github.io/notebooks/bayesian-optimization_files/bayesian-optimization_21_0.png)\n",
    "\n",
    "The observations supporting the construction of the surrogate function are not randomly sparse around, because, through an acquisition function (in a gaussian processes it is a function guiding the selection of the next evaluation points), they have been picked as the most useful examples in order to guess how to minimize the cost function.\n",
    "\n",
    "In respect of a random optimization, a bayesian optimization is more of an educated guess, then, first sampling randomly, but then focussing on the most important combination of hyper-parameters in order to figure out, first the surrogate function of the cost function, then the global minimum of the cost function:\n",
    "\n",
    "![figure_2](https://scikit-optimize.github.io/notebooks/bayesian-optimization_files/bayesian-optimization_18_1.png)\n",
    "\n",
    "Gaussian process (GP) is one of the possible ways to build a surrogate function: it consists of a distribution on functions.\n",
    "Originally GPs were developed to help search for gold ([kriging](https://en.wikipedia.org/wiki/Kriging)). Please note that the approach is closely related to the statistical ideas in the optimal design of experiments.\n",
    "In a gaussian process, based on a distribution of functions resembling the true cost function, the alogorithm operates in:\n",
    "\n",
    "* Exploration -> seeking points and areas on the optimization surface with high variance\n",
    "* Exploitation -> seeking points with low mean\n",
    "\n",
    "This is done by a second, specialized function, the acquisition function.\n",
    "\n",
    "Other approaches are 1) ensembles of decision trees 2) Tree of Parzen Estimators (TPE used by [Hyperopt](http://hyperopt.github.io/hyperopt/) \n",
    "another Bayesian optimization package package) \n",
    "\n",
    "Gaussian Processes are just models, and they're much more like k-nearest neighbors and linear regression than may at first be apparent. If you want to understand more of GPs, you can read the post: [https://planspace.org/20181226-gaussian_processes_are_not_so_fancy/](https://planspace.org/20181226-gaussian_processes_are_not_so_fancy)by Aaron Schumacher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "cca6073d5c1d67dd11ab6b8f77800d77ff044cca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BayesSearchCV_GP took 143.53 seconds,  candidates checked: 40, best CV score: 0.928 ± 0.062\n",
      "Best parameters:\n",
      "{'learning_rate': 0.7184419933944712,\n",
      " 'max_features': 'log2',\n",
      " 'min_samples_leaf': 10,\n",
      " 'min_samples_split': 3,\n",
      " 'n_estimators': 347,\n",
      " 'subsample': 1.0}\n",
      "\n",
      "BayesSearchCV_RF took 108.61 seconds,  candidates checked: 40, best CV score: 0.945 ± 0.051\n",
      "Best parameters:\n",
      "{'learning_rate': 0.6049627162690525,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 9,\n",
      " 'min_samples_split': 10,\n",
      " 'n_estimators': 485,\n",
      " 'subsample': 0.6024816853254407}\n",
      "\n",
      "BayesSearchCV_ET took 93.15 seconds,  candidates checked: 40, best CV score: 0.933 ± 0.059\n",
      "Best parameters:\n",
      "{'learning_rate': 0.39752021916372565,\n",
      " 'max_features': None,\n",
      " 'min_samples_leaf': 6,\n",
      " 'min_samples_split': 9,\n",
      " 'n_estimators': 78,\n",
      " 'subsample': 0.7687211202072857}\n",
      "\n",
      "BayesSearchCV_GBRT took 73.68 seconds,  candidates checked: 40, best CV score: 0.933 ± 0.068\n",
      "Best parameters:\n",
      "{'learning_rate': 0.42127830500668484,\n",
      " 'max_features': 'log2',\n",
      " 'min_samples_leaf': 4,\n",
      " 'min_samples_split': 8,\n",
      " 'n_estimators': 377,\n",
      " 'subsample': 0.5657650066445118}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# also BayesSearchCV needs to work on the distributions of the experiments but it is less sensible to them\n",
    "\n",
    "search_spaces = {\"learning_rate\": Real(0.01, 1.0),\n",
    "                 \"n_estimators\": Integer(10, 500),\n",
    "                 \"subsample\": Real(0.5, 1.0),\n",
    "                 \"min_samples_split\": Integer(2, 10),\n",
    "                 \"min_samples_leaf\": Integer(1, 10),\n",
    "                 \"max_features\": Categorical(categories=['sqrt', 'log2', None])}\n",
    "\n",
    "for baseEstimator in ['GP', 'RF', 'ET', 'GBRT']:\n",
    "    opt = BayesSearchCV(clf,\n",
    "                        search_spaces,\n",
    "                        scoring=avg_prec,\n",
    "                        cv=skf,\n",
    "                        n_iter=40,\n",
    "                        n_jobs=-1,\n",
    "                        return_train_score=False,\n",
    "                        optimizer_kwargs={'base_estimator': baseEstimator},\n",
    "                        random_state=4)\n",
    "    \n",
    "    best_params = report_perf(opt, X, y_bin,'BayesSearchCV_'+baseEstimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controlling the time cost of Bayesian optimization\n",
    "\n",
    "Running a single LightGBM model could take long time and in a Kaggle competition time is often a luxury. \n",
    "\n",
    "*DeadlineStopper* and *DeltaXStopper* are skopt callbacks that control the total time spent and the improvement of a BayesSearchCV (in our implementation to be called with *report_perf*, using the parameter *callbacks=[]*). \n",
    "\n",
    "Anyway, sometimes it is easier to control manually the optimization steps, hence the usage of low-level optimizers. \n",
    "\n",
    "We start defining a custom callback, using a different approach to search spaces (a list instead of a dictionary), and to manually create our objective function to be minimized.\n",
    "\n",
    "In our custom callback, we print the last evaluation point (so you know what's happening) and the best score and parameters foudn up so far. We also record the list of explored points (*x0*) and their relative results (*y0*). This will help us to reprise the learning at a later time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "4218521e57192fa23c0ab536653f3d07d04e2402"
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "def onstep(res):\n",
    "    global counter\n",
    "    x0 = res.x_iters   # List of input points\n",
    "    y0 = res.func_vals # Evaluation of input points\n",
    "    print('Last eval: ', x0[-1], \n",
    "          ' - Score ', y0[-1])\n",
    "    print('Current iter: ', counter, \n",
    "          ' - Score ', res.fun, \n",
    "          ' - Args: ', res.x)\n",
    "    joblib.dump((x0, y0), 'checkpoint.pkl') # Saving a checkpoint to disk\n",
    "    counter += 1\n",
    "\n",
    "# Our search space\n",
    "dimensions = [Real(0.01, 1.0, name=\"learning_rate\"),\n",
    "              Integer(10, 500, name=\"n_estimators\"),\n",
    "              Real(0.5, 1.0, name=\"subsample\"),\n",
    "              Integer(2, 10, name=\"min_samples_split\"),\n",
    "              Integer(1, 10, name=\"min_samples_leaf\"),\n",
    "              Categorical(categories=['sqrt', 'log2', None], name=\"max_features\")]\n",
    "\n",
    "# The objective function to be minimized\n",
    "def make_objective(model, X, y, space, cv, scoring):\n",
    "    # This decorator converts your objective function with named arguments into one that\n",
    "    # accepts a list as argument, while doing the conversion automatically.\n",
    "    @use_named_args(space) \n",
    "    def objective(**params):\n",
    "        model.set_params(**params)\n",
    "        return -np.mean(cross_val_score(model, \n",
    "                                        X, y, \n",
    "                                        cv=cv, \n",
    "                                        n_jobs=-1,\n",
    "                                        scoring=scoring))\n",
    "\n",
    "    return objective\n",
    "\n",
    "objective = make_objective(clf,\n",
    "                           X, y_bin,\n",
    "                           space=dimensions,\n",
    "                           cv=skf,\n",
    "                           scoring=avg_prec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different low-level optimizers that can be used for the purpose:\n",
    "* **gp_minimize** Bayesian optimization using Gaussian Processes.\n",
    "* **forest_minimize** Sequential optimisation using decision trees\n",
    "* **gbrt_minimize** Sequential optimization using gradient boosted trees\n",
    "* **dummy_minimize** Random search by uniform sampling within the given bounds (a replacement for Scikit-learn's RandomSearch)\n",
    "\n",
    "Each optimizer has its own parameters, so they cannot be just automatically switched, though they share most of the key parameters.\n",
    "\n",
    "Here we encounter also a new parameter, **acq_func**, useful for defining how the acquisition function should behave, that is, if to take as minimum the lower confidence bound, the minimum expected value or probability (the suggested default is usually a good choice)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "88361c5f2f425f8cbf334d8b494692405e8c0b86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last eval:  [0.7049433514242192, 368, 0.8187395776032731, 8, 9, None]  - Score  -0.9053516521792384\n",
      "Current iter:  0  - Score  -0.9053516521792384  - Args:  [0.7049433514242192, 368, 0.8187395776032731, 8, 9, None]\n",
      "Last eval:  [0.24229127648570348, 16, 0.7320268705932458, 8, 2, 'sqrt']  - Score  -0.8892368868276309\n",
      "Current iter:  1  - Score  -0.9053516521792384  - Args:  [0.7049433514242192, 368, 0.8187395776032731, 8, 9, None]\n",
      "Last eval:  [0.09808017080760961, 436, 0.721036739670265, 3, 4, 'log2']  - Score  -0.9234068462086364\n",
      "Current iter:  2  - Score  -0.9234068462086364  - Args:  [0.09808017080760961, 436, 0.721036739670265, 3, 4, 'log2']\n",
      "Last eval:  [0.010263121534902437, 442, 0.972884851277688, 9, 2, 'sqrt']  - Score  -0.9049683094683093\n",
      "Current iter:  3  - Score  -0.9234068462086364  - Args:  [0.09808017080760961, 436, 0.721036739670265, 3, 4, 'log2']\n",
      "Last eval:  [0.8875384524348883, 205, 0.9771110531385809, 8, 8, None]  - Score  -0.8977127929278959\n",
      "Current iter:  4  - Score  -0.9234068462086364  - Args:  [0.09808017080760961, 436, 0.721036739670265, 3, 4, 'log2']\n",
      "Last eval:  [0.9408844987095382, 278, 0.7733463688856329, 7, 2, None]  - Score  -0.7844031794538096\n",
      "Current iter:  5  - Score  -0.9234068462086364  - Args:  [0.09808017080760961, 436, 0.721036739670265, 3, 4, 'log2']\n",
      "Last eval:  [0.3343579995412422, 367, 0.5956932474515344, 8, 9, 'log2']  - Score  -0.9143859278549682\n",
      "Current iter:  6  - Score  -0.9234068462086364  - Args:  [0.09808017080760961, 436, 0.721036739670265, 3, 4, 'log2']\n",
      "Last eval:  [0.7879021133849456, 252, 0.75143818812381, 8, 5, 'log2']  - Score  -0.5935980587554553\n",
      "Current iter:  7  - Score  -0.9234068462086364  - Args:  [0.09808017080760961, 436, 0.721036739670265, 3, 4, 'log2']\n",
      "Last eval:  [0.8164027897144066, 346, 0.6236194997341288, 6, 7, 'sqrt']  - Score  -0.8692463647463649\n",
      "Current iter:  8  - Score  -0.9234068462086364  - Args:  [0.09808017080760961, 436, 0.721036739670265, 3, 4, 'log2']\n",
      "Last eval:  [0.7856253596534273, 448, 0.7012180692236435, 9, 8, 'sqrt']  - Score  -0.753150122881838\n",
      "Current iter:  9  - Score  -0.9234068462086364  - Args:  [0.09808017080760961, 436, 0.721036739670265, 3, 4, 'log2']\n"
     ]
    }
   ],
   "source": [
    "gp_round = gp_minimize(func=objective,\n",
    "                       dimensions=dimensions,\n",
    "                       acq_func='gp_hedge', # Defining what to minimize \n",
    "                       n_calls=10,\n",
    "                       callback=[onstep],\n",
    "                       random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "de1369d11ea6a3bc79328bc2f2e14dd2621f4686"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last eval:  [0.7856253596534273, 448, 0.7012180692236435, 9, 8, 'sqrt']  - Score  -0.753150122881838\n",
      "Current iter:  10  - Score  -0.9234068462086364  - Args:  [0.09808017080760961, 436, 0.721036739670265, 3, 4, 'log2']\n",
      "Last eval:  [0.5969161720427683, 424, 0.9289728088113784, 9, 7, 'log2']  - Score  -0.9433474858474857\n",
      "Current iter:  11  - Score  -0.9433474858474857  - Args:  [0.5969161720427683, 424, 0.9289728088113784, 9, 7, 'log2']\n",
      "Last eval:  [0.3045592604790276, 38, 0.6363281472900566, 6, 8, 'log2']  - Score  -0.9160759254036765\n",
      "Current iter:  12  - Score  -0.9433474858474857  - Args:  [0.5969161720427683, 424, 0.9289728088113784, 9, 7, 'log2']\n",
      "Last eval:  [0.39885694813982153, 420, 0.6686980802086342, 7, 4, None]  - Score  -0.8992387417077818\n",
      "Current iter:  13  - Score  -0.9433474858474857  - Args:  [0.5969161720427683, 424, 0.9289728088113784, 9, 7, 'log2']\n",
      "Last eval:  [0.14894727260851873, 436, 0.7368040226368553, 8, 6, None]  - Score  -0.9172835775335775\n",
      "Current iter:  14  - Score  -0.9433474858474857  - Args:  [0.5969161720427683, 424, 0.9289728088113784, 9, 7, 'log2']\n",
      "Last eval:  [0.7234263281786577, 295, 0.7686866147245054, 8, 2, 'log2']  - Score  -0.9011155635306421\n",
      "Current iter:  15  - Score  -0.9433474858474857  - Args:  [0.5969161720427683, 424, 0.9289728088113784, 9, 7, 'log2']\n",
      "Last eval:  [0.1944690198934924, 371, 0.6082751772121859, 3, 4, 'sqrt']  - Score  -0.9163157411434923\n",
      "Current iter:  16  - Score  -0.9433474858474857  - Args:  [0.5969161720427683, 424, 0.9289728088113784, 9, 7, 'log2']\n",
      "Last eval:  [0.23009817436907185, 199, 0.9512992377647025, 6, 7, None]  - Score  -0.901292475498358\n",
      "Current iter:  17  - Score  -0.9433474858474857  - Args:  [0.5969161720427683, 424, 0.9289728088113784, 9, 7, 'log2']\n",
      "Last eval:  [0.10828754685538415, 485, 0.8265700178989689, 3, 4, None]  - Score  -0.9187638287638288\n",
      "Current iter:  18  - Score  -0.9433474858474857  - Args:  [0.5969161720427683, 424, 0.9289728088113784, 9, 7, 'log2']\n",
      "Last eval:  [0.6117523620283132, 169, 0.5192127132363674, 7, 10, 'log2']  - Score  -0.8649301045063078\n",
      "Current iter:  19  - Score  -0.9433474858474857  - Args:  [0.5969161720427683, 424, 0.9289728088113784, 9, 7, 'log2']\n",
      "Last eval:  [0.6387082848675283, 498, 0.7909251647192672, 5, 5, 'log2']  - Score  -0.7693444638581505\n",
      "Current iter:  20  - Score  -0.9433474858474857  - Args:  [0.5969161720427683, 424, 0.9289728088113784, 9, 7, 'log2']\n"
     ]
    }
   ],
   "source": [
    "x0, y0 = joblib.load('checkpoint.pkl')\n",
    "\n",
    "gp_round = gp_minimize(func=objective,\n",
    "                       x0=x0,              # already examined values for x\n",
    "                       y0=y0,              # observed values for x0\n",
    "                       dimensions=dimensions,\n",
    "                       acq_func='gp_hedge', # Expected Improvement.\n",
    "                       n_calls=10,\n",
    "                       callback=[onstep],\n",
    "                       random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "2826f2d0f47b973052a81643498617f3b6f1cbd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5969161720427683, 424, 0.9289728088113784, 9, 7, 'log2'] -0.9433474858474857\n"
     ]
    }
   ],
   "source": [
    "best_parameters = gp_round.x\n",
    "best_result = gp_round.fun\n",
    "print(best_parameters, best_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, just keep in mind a few points from the workshop:\n",
    "\n",
    "* Bayesian Optimization has its own hyper-parameters (therefore use defaults, [unless you know what you doing](https://i0.kym-cdn.com/entries/icons/original/000/008/342/ihave.jpg))\n",
    "\n",
    "* Experiments are run sequentially (skopt can leverage some parallelism, though), having multiple cores is helpful for your learning algorithm,but Bayesian Optimization will always be slower than Random Search. Use it only when needed.\n",
    "\n",
    "* Packages are not all that friendly (hence the workshop :-)) but you can reuse some simple wrappers re-adaptable to being used in different Kaggle competitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1a82e07c7940ff0587db89b8ceba55be99564fea"
   },
   "source": [
    "# A Practical Example: Optimizing LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "50ecf1e5429ea895469fb93055c033424b52f71f"
   },
   "source": [
    "The high-performance [LightGBM](https://github.com/Microsoft/LightGBM) algorithm is capable of being distributed and of fast-handling large amounts of data. It has been developed by a team at Microsoft as an open source project on GitHub (there is also an [academic paper](https:/ / papers. nips. cc/ paper/ 6907- lightgbm- a- highly- efficientgradient-boosting- decision- tree)). \n",
    "\n",
    "LightGBM is based on decision trees, as well as XGBoost, yet it follows a different strategy.\n",
    "Whereas XGBoost uses decision trees to split on a variable and exploring different cuts at that variable (the level-wise tree growth strategy), LightGBM concentrates on a split and goes on splitting from there in order to achieve a better fitting (this is the leaf-wise tree\n",
    "growth strategy). This allows LightGBM to reach first and fast a good fit of the data, and to generate alternative solutions compared to XGBoost (which is good, if you expect to blend, i.e. average, the two solutions together in order to reduce the variance of the estimated). Algorithmically talking, figuring out as a graph the structures of cuts operated by a decision tree, XGBoost peruses a breadth-first search (BFS), whereas LightGBM a depthfirst search (DFS).\n",
    "\n",
    "Tuning LightGBM may appear daunting with more than a [hundred parameters](https://github.com/Microsoft/LightGBM/blob/master/docs/Parameters.rst) (also to be found [here](https://lightgbm.readthedocs.io/en/latest/Parameters.html)) to fix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "2d1d9fbdb869a040c93f1ffacf59c14f3e473264"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM took 294.95 seconds,  candidates checked: 27, best CV score: 0.925 ± 0.041\n",
      "Best parameters:\n",
      "{'colsample_bytree': 0.7118443850876301,\n",
      " 'learning_rate': 1.0,\n",
      " 'max_bin': 58926,\n",
      " 'max_depth': 186,\n",
      " 'min_child_samples': 0,\n",
      " 'min_child_weight': 6,\n",
      " 'n_estimators': 10,\n",
      " 'num_leaves': 500,\n",
      " 'reg_alpha': 0.0016095545235147851,\n",
      " 'reg_lambda': 1e-09,\n",
      " 'scale_pos_weight': 214.2637475974076,\n",
      " 'subsample': 1.0,\n",
      " 'subsample_for_bin': 250671,\n",
      " 'subsample_freq': 7}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = lgb.LGBMClassifier(boosting_type='gbdt',\n",
    "                         class_weight='balanced',\n",
    "                         objective='binary',\n",
    "                         n_jobs=1, \n",
    "                         verbose=0)\n",
    "\n",
    "search_spaces = {\n",
    "        'learning_rate': Real(0.01, 1.0, 'log-uniform'),\n",
    "        'num_leaves': Integer(2, 500),\n",
    "        'max_depth': Integer(0, 500),\n",
    "        'min_child_samples': Integer(0, 200),\n",
    "        'max_bin': Integer(100, 100000),\n",
    "        'subsample': Real(0.01, 1.0, 'uniform'),\n",
    "        'subsample_freq': Integer(0, 10),\n",
    "        'colsample_bytree': Real(0.01, 1.0, 'uniform'),\n",
    "        'min_child_weight': Integer(0, 10),\n",
    "        'subsample_for_bin': Integer(100000, 500000),\n",
    "        'reg_lambda': Real(1e-9, 1000, 'log-uniform'),\n",
    "        'reg_alpha': Real(1e-9, 1.0, 'log-uniform'),\n",
    "        'scale_pos_weight': Real(1e-6, 500, 'log-uniform'),\n",
    "        'n_estimators': Integer(10, 10000)        \n",
    "        }\n",
    "\n",
    "opt = BayesSearchCV(clf,\n",
    "                    search_spaces,\n",
    "                    scoring=avg_prec,\n",
    "                    cv=skf,\n",
    "                    n_iter=40,\n",
    "                    n_jobs=-1,\n",
    "                    return_train_score=False,\n",
    "                    refit=True,\n",
    "                    optimizer_kwargs={'base_estimator': 'GP'},\n",
    "                    random_state=22)\n",
    "    \n",
    "best_params = report_perf(opt, X, y_bin,'LightGBM', \n",
    "                          callbacks=[DeltaXStopper(0.001), \n",
    "                                     DeadlineStopper(60*5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "5e4e99b9b5c3d22ae5afdf8e2437c66e3c84cefc"
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "\n",
    "clf = lgb.LGBMClassifier(boosting_type='gbdt',\n",
    "                         class_weight='balanced',\n",
    "                         objective='binary',\n",
    "                         n_jobs=1, \n",
    "                         verbose=0)\n",
    "\n",
    "dimensions = [Real(0.01, 1.0, 'log-uniform', name='learning_rate'),\n",
    "              Integer(2, 500, name='num_leaves'),\n",
    "              Integer(0, 500, name='max_depth'),\n",
    "              Integer(0, 200, name='min_child_samples'),\n",
    "              Integer(100, 100000, name='max_bin'),\n",
    "              Real(0.01, 1.0, 'uniform', name='subsample'),\n",
    "              Integer(0, 10, name='subsample_freq'),\n",
    "              Real(0.01, 1.0, 'uniform', name='colsample_bytree'),\n",
    "              Integer(0, 10, name='min_child_weight'),\n",
    "              Integer(100000, 500000, name='subsample_for_bin'),\n",
    "              Real(1e-9, 1000, 'log-uniform', name='reg_lambda'),\n",
    "              Real(1e-9, 1.0, 'log-uniform', name='reg_alpha'),\n",
    "              Real(1e-6, 500, 'log-uniform', name='scale_pos_weight'),\n",
    "              Integer(10, 10000, name='n_estimators')]\n",
    "\n",
    "objective = make_objective(clf,\n",
    "                           X, y_bin,\n",
    "                           space=dimensions,\n",
    "                           cv=skf,\n",
    "                           scoring=avg_prec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "6de6368797702d3944fcb35082984be4910cfbbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last eval:  [0.02848906260926589, 161, 489, 91, 30870, 0.2712321306475197, 1, 0.4251784883123234, 0, 311106, 26.645108834238002, 9.496274156079435e-07, 0.002619348997981635, 6747]  - Score  -0.1007765482430596\n",
      "Current iter:  0  - Score  -0.1007765482430596  - Args:  [0.02848906260926589, 161, 489, 91, 30870, 0.2712321306475197, 1, 0.4251784883123234, 0, 311106, 26.645108834238002, 9.496274156079435e-07, 0.002619348997981635, 6747]\n",
      "Last eval:  [0.2211233200687724, 348, 173, 186, 26332, 0.7532551005821186, 3, 0.8527816404253235, 2, 416305, 178.4645772067005, 1.095045483795558e-05, 0.0022184775182806722, 3581]  - Score  -0.1007765482430596\n",
      "Current iter:  1  - Score  -0.1007765482430596  - Args:  [0.02848906260926589, 161, 489, 91, 30870, 0.2712321306475197, 1, 0.4251784883123234, 0, 311106, 26.645108834238002, 9.496274156079435e-07, 0.002619348997981635, 6747]\n",
      "Last eval:  [0.02436190508232831, 52, 241, 123, 95130, 0.3552688118981681, 3, 0.45204083607316786, 10, 171831, 5.159278367758748e-08, 5.643816697371209e-09, 0.05759462438024217, 3109]  - Score  -0.1007765482430596\n",
      "Current iter:  2  - Score  -0.1007765482430596  - Args:  [0.02848906260926589, 161, 489, 91, 30870, 0.2712321306475197, 1, 0.4251784883123234, 0, 311106, 26.645108834238002, 9.496274156079435e-07, 0.002619348997981635, 6747]\n",
      "Last eval:  [0.036765413368033246, 461, 217, 84, 98624, 0.7956342983145053, 3, 0.307335685328769, 5, 349302, 0.07650111834271556, 2.1489283637488493e-07, 0.0033482236099689746, 6676]  - Score  -0.1007765482430596\n",
      "Current iter:  3  - Score  -0.1007765482430596  - Args:  [0.02848906260926589, 161, 489, 91, 30870, 0.2712321306475197, 1, 0.4251784883123234, 0, 311106, 26.645108834238002, 9.496274156079435e-07, 0.002619348997981635, 6747]\n",
      "Last eval:  [0.07568374719544937, 391, 342, 147, 48993, 0.9544514920010971, 2, 0.6526276640345808, 6, 447413, 5.290980835305423e-08, 1.0743335173861622e-05, 0.5428137986259088, 6620]  - Score  -0.5184708747168858\n",
      "Current iter:  4  - Score  -0.5184708747168858  - Args:  [0.07568374719544937, 391, 342, 147, 48993, 0.9544514920010971, 2, 0.6526276640345808, 6, 447413, 5.290980835305423e-08, 1.0743335173861622e-05, 0.5428137986259088, 6620]\n",
      "Last eval:  [0.47548110586714587, 267, 20, 19, 79910, 0.4083645994626534, 7, 0.5069042541174733, 4, 414327, 260.63557428396865, 2.0969660888077091e-07, 0.03329930701856852, 8682]  - Score  -0.1007765482430596\n",
      "Current iter:  5  - Score  -0.5184708747168858  - Args:  [0.07568374719544937, 391, 342, 147, 48993, 0.9544514920010971, 2, 0.6526276640345808, 6, 447413, 5.290980835305423e-08, 1.0743335173861622e-05, 0.5428137986259088, 6620]\n",
      "Last eval:  [0.08293216060373823, 57, 400, 100, 19798, 0.8773143587445786, 3, 0.24456885606236203, 4, 254361, 6.267403019545165e-07, 3.8195699908270306e-08, 0.0029281270257610842, 8418]  - Score  -0.4932644929680439\n",
      "Current iter:  6  - Score  -0.5184708747168858  - Args:  [0.07568374719544937, 391, 342, 147, 48993, 0.9544514920010971, 2, 0.6526276640345808, 6, 447413, 5.290980835305423e-08, 1.0743335173861622e-05, 0.5428137986259088, 6620]\n",
      "Last eval:  [0.3745080737521305, 321, 248, 95, 47476, 0.9441414325971597, 7, 0.051157271586472694, 8, 305360, 1.3839138544254465e-07, 3.372981975245276e-06, 149.1039700696765, 2137]  - Score  -0.6843172927234125\n",
      "Current iter:  7  - Score  -0.6843172927234125  - Args:  [0.3745080737521305, 321, 248, 95, 47476, 0.9441414325971597, 7, 0.051157271586472694, 8, 305360, 1.3839138544254465e-07, 3.372981975245276e-06, 149.1039700696765, 2137]\n",
      "Last eval:  [0.42324568665282264, 203, 293, 37, 81251, 0.5850389357102426, 4, 0.8001005930590471, 5, 267367, 1.6977743026226538e-09, 0.0014254878711280096, 0.01761769861494498, 2214]  - Score  -0.3623421719518635\n",
      "Current iter:  8  - Score  -0.6843172927234125  - Args:  [0.3745080737521305, 321, 248, 95, 47476, 0.9441414325971597, 7, 0.051157271586472694, 8, 305360, 1.3839138544254465e-07, 3.372981975245276e-06, 149.1039700696765, 2137]\n",
      "Last eval:  [0.12620776965554556, 262, 244, 53, 52776, 0.9846210751689275, 6, 0.6332613654964749, 9, 485008, 0.010500201470771384, 3.0484544036415625e-08, 0.0025495207496935173, 9446]  - Score  -0.1007765482430596\n",
      "Current iter:  9  - Score  -0.6843172927234125  - Args:  [0.3745080737521305, 321, 248, 95, 47476, 0.9441414325971597, 7, 0.051157271586472694, 8, 305360, 1.3839138544254465e-07, 3.372981975245276e-06, 149.1039700696765, 2137]\n"
     ]
    }
   ],
   "source": [
    "gp_round = gp_minimize(func=objective,\n",
    "                       dimensions=dimensions,\n",
    "                       acq_func='gp_hedge',\n",
    "                       n_calls=10, # Minimum is 10 calls\n",
    "                       callback=[onstep],\n",
    "                       random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "4ba09b481009f9e87378aae6c4f17024878d31e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.12324384123200562, 414, 117, 41, 95838, 0.7684912331585715, 2, 0.47468749760756623, 5, 168945, 3.8096250370376167e-06, 2.625825905482537e-07, 51.89197450218198, 5038] -0.8572449837163276\n"
     ]
    }
   ],
   "source": [
    "x0, y0 = joblib.load('checkpoint.pkl')\n",
    "\n",
    "gp_round = gp_minimize(func=objective,\n",
    "                       x0=x0,              # already examined values for x\n",
    "                       y0=y0,              # observed values for x0\n",
    "                       dimensions=dimensions,\n",
    "                       acq_func='gp_hedge', # Expected Improvement.\n",
    "                       n_calls=10,\n",
    "                       #callback=[onstep],\n",
    "                       random_state=3)\n",
    "\n",
    "best_parameters = gp_round.x\n",
    "best_result = gp_round.fun\n",
    "print(best_parameters, best_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "561467430472a5562fa25d04d2e3c4f311800ee6"
   },
   "source": [
    "# Practical example: Optimizing XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[XGBoost](https://github.com/dmlc/XGBoost) stands for eXtreme Gradient Boosting, an open source project that is not part of Scikit-learn, though recently it has been expanded by a Scikit-Learn wrapper interface that renders using models based on XGBoost more integrated into your data pipeline.\n",
    "\n",
    "The XGBoost algorithm has gained recently gained momentum and popularity in datascience competitions such as Kaggle and the KDD-cup 2015. As the authors (Tianqui Chen, Tong He, and Carlos Guestrin) report on papers they wrote on the algorithm, among 29 challenges held on Kaggle during 2015, 17 winning solutions used XGBoost as a standalone solution or as part of an ensemble of multiple different models.\n",
    "\n",
    "Apart from the successful performances in both accuracy and computational efficiency, XGBoost is also a scalable solution under different points of view. XGBoost represents a new generation of GBM algorithms thanks to important tweaks to the initial tree boost GBM algorithm:\n",
    "\n",
    "* A sparse-aware algorithm; it can leverage sparse matrices, saving both memory (no need for dense matrices) and computation time (zero values are handled in a special way).\n",
    "* Approximate tree learning (weighted quantile sketch), which bears similar results but in much less time than the classical complete explorations of possible branch cuts.\n",
    "* Parallel computing on a single machine (using multi-threading in the phase of the search for the best split) and similarly distributed computations on multiple ones. \n",
    "* Out-of-core computations on a single machine leveraging a data storage solution called Column Block. This arranges data on a disk by columns, thus saving time by pulling data from the disk as the optimization algorithm (which works on column vectors) expects it.\n",
    "* XGBoost can also deal with missing data in an effective way. Other tree ensembles based on standard decision trees require missing data first to be imputed using an off-scale value, such as a negative number, in order to develop an appropriate branching of the tree to deal with missing values.\n",
    "\n",
    "As for as XGBoost's [parameters](https://xgboost.readthedocs.io/en/latest/parameter.html), we have decided to work on 13 key ones.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "c899ead400eb74ce87a9176aca1a47d1fcbf5946"
   },
   "outputs": [],
   "source": [
    "clf = xgb.XGBClassifier(\n",
    "        n_jobs = 1,\n",
    "        objective = 'binary:logistic',\n",
    "        silent=1,\n",
    "        tree_method='approx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "71b250ac2647634c4528c4c3a55de66b66127144"
   },
   "outputs": [],
   "source": [
    "search_spaces = {'learning_rate': Real(0.01, 1.0, 'log-uniform'),\n",
    "                 'min_child_weight': Integer(0, 10),\n",
    "                 'max_depth': Integer(0, 50),\n",
    "                 'max_delta_step': Integer(0, 20),\n",
    "                 'subsample': Real(0.01, 1.0, 'uniform'),\n",
    "                 'colsample_bytree': Real(0.01, 1.0, 'uniform'),\n",
    "                 'colsample_bylevel': Real(0.01, 1.0, 'uniform'),\n",
    "                 'reg_lambda': Real(1e-9, 1000, 'log-uniform'),\n",
    "                 'reg_alpha': Real(1e-9, 1.0, 'log-uniform'),\n",
    "                 'gamma': Real(1e-9, 0.5, 'log-uniform'),\n",
    "                 'min_child_weight': Integer(0, 5),\n",
    "                 'n_estimators': Integer(50, 100),\n",
    "                 'scale_pos_weight': Real(1e-6, 500, 'log-uniform')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "5ef9b0986d0677d866304c50a3c503b0bc4a4f40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost took 184.54 seconds,  candidates checked: 40, best CV score: 0.933 ± 0.044\n",
      "Best parameters:\n",
      "{'colsample_bylevel': 0.679420835129346,\n",
      " 'colsample_bytree': 0.9302387873166793,\n",
      " 'gamma': 1.5547111219661747e-09,\n",
      " 'learning_rate': 0.1893718239414261,\n",
      " 'max_delta_step': 3,\n",
      " 'max_depth': 11,\n",
      " 'min_child_weight': 0,\n",
      " 'n_estimators': 100,\n",
      " 'reg_alpha': 1.0,\n",
      " 'reg_lambda': 2.9991940692965716e-08,\n",
      " 'scale_pos_weight': 41.04156729321037,\n",
      " 'subsample': 0.904711876381536}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "opt = BayesSearchCV(clf,\n",
    "                    search_spaces,\n",
    "                    scoring=avg_prec,\n",
    "                    cv=skf,\n",
    "                    n_iter=40,\n",
    "                    n_jobs=-1,\n",
    "                    return_train_score=False,\n",
    "                    refit=True,\n",
    "                    optimizer_kwargs={'base_estimator': 'GP'},\n",
    "                    random_state=22)\n",
    "    \n",
    "best_params = report_perf(opt, X, y_bin,'XGBoost',                           \n",
    "                          callbacks=[DeltaXStopper(0.001), \n",
    "                                     DeadlineStopper(60*5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f8d79cc09922ea88e960afdd9fadc021dc71ce1c"
   },
   "source": [
    "# Practical Example: Optimizing CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In July 2017, another interesting GBM algorithm was made public by Yandex, the Russian search engine: it is [CatBoost](https://catboost.yandex/), whose name comes from putting together the two words Category and Boosting. In fact, its strongest point is the capability of handling categorical variables, which actually make the most of information in most relational databases, by adopting a mixed strategy of one-hot-encoding and mean encoding (a way to express categorical levels by assigning them an appropriate numeric value for the problem at hand; more on that later).\n",
    "\n",
    "The idea used by CatBoost to encode the categorical variables is not new, but it has been a kind of feature engineering used various times, mostly in data science competitions like at Kaggle’s. Mean encoding, also known as likelihood encoding, impact coding, or target coding, is simply a way to transform your labels into a number based on their association with the target variable. If you have a regression, you could transform labels based on the mean target value typical of that level; if it is a classification, it is simply the probability of classification of your target given that label (probability of your target, conditional on each category value). It may appear as a simple and smart feature engineering trick, but actually, it has side effects, mostly in terms of overfitting because you are taking information from the target into your predictors.\n",
    "\n",
    "CatBoost has quite a few [parameters](https://tech.yandex.com/catboost/doc/dg/concepts/python-reference_parameters-list-docpage/#python-reference_parameters-list), we have delimited our search to the 9 most important ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "24c3a9bdb05e58c83d3ffd51be6163f43d26f1aa"
   },
   "outputs": [],
   "source": [
    "clf = CatBoostClassifier(loss_function='Logloss',\n",
    "                         verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "277816facc46abcc9581dd6a485cd519130656b2"
   },
   "outputs": [],
   "source": [
    "search_spaces = {'iterations': Integer(10, 100),\n",
    "                 'depth': Integer(1, 8),\n",
    "                 'learning_rate': Real(0.01, 1.0, 'log-uniform'),\n",
    "                 'random_strength': Real(1e-9, 10, 'log-uniform'),\n",
    "                 'bagging_temperature': Real(0.0, 1.0),\n",
    "                 'border_count': Integer(1, 255),\n",
    "                 'ctr_border_count': Integer(1, 255),\n",
    "                 'l2_leaf_reg': Integer(2, 30),\n",
    "                 'scale_pos_weight':Real(0.01, 1.0, 'uniform')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "816a4893a523ec3d9a155d6a095e0c023666b5d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost took 230.68 seconds,  candidates checked: 21, best CV score: 0.915 ± 0.061\n",
      "Best parameters:\n",
      "{'bagging_temperature': 0.08479426613384157,\n",
      " 'border_count': 170,\n",
      " 'ctr_border_count': 64,\n",
      " 'depth': 1,\n",
      " 'iterations': 40,\n",
      " 'l2_leaf_reg': 3,\n",
      " 'learning_rate': 1.0,\n",
      " 'random_strength': 0.0002627408070624004,\n",
      " 'scale_pos_weight': 1.0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "opt = BayesSearchCV(clf,\n",
    "                    search_spaces,\n",
    "                    scoring=avg_prec,\n",
    "                    cv=skf,\n",
    "                    n_iter=40,\n",
    "                    n_jobs=1,  # use just 1 job with CatBoost in order to avoid segmentation fault\n",
    "                    return_train_score=False,\n",
    "                    refit=True,\n",
    "                    optimizer_kwargs={'base_estimator': 'GP'},\n",
    "                    random_state=22)\n",
    "\n",
    "best_params = report_perf(opt, X, y_bin,'CatBoost', \n",
    "                          callbacks=[DeltaXStopper(0.001), \n",
    "                                     DeadlineStopper(60*5)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
